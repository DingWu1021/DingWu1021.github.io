<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="We present DAGait, a universal data alignment strategy for gait recognition, to alleviate spatiotemporal distribution inconsistencies.">
  <meta property="og:title" content="DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition" />
  <meta property="og:description"
    content="We present DAGait, a universal data alignment strategy for gait recognition, to alleviate spatiotemporal distribution inconsistencies." />
  <meta property="og:url" content="https://DingWu1021.github.io/DAGait" />
  <meta property="og:image" content="static/images/baseline.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="400" />

  <meta name="twitter:title" content="DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition">
  <meta name="twitter:description" content="We present DAGait, a universal data alignment strategy for gait recognition, to alleviate spatiotemporal distribution inconsistencies.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/baseline.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Gait Recognition, Alignment, deep learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>DAGait</title>
  <!-- Favicon generated by https://redketchup.io/favicon-generator -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="static/images/favicon/site.webmanifest">

  <link rel="stylesheet" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bootstrap.bundle.min.js"></script>
</head>

<body>
  <div class="container">
    <!-- Title -->
    <h1 class="pt-5 title mb-4">DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition</h1>

    <div class="mb-4" style="text-align: center; font-size: 1.5rem;">
      ICME 2025
    </div>

    <div class="mb-2" style="font-size: larger; text-align: center;">
      <span class="author-block">
        <a href="https://github.com/DingWu1021/DAGait">Zhengxian Wu</a><sup>1*</sup></span>,&nbsp;
      <span class="author-block">
        <a href="https://xingyoujun.github.io/">Chuanrui Zhang</a><sup>1*</sup></span>,&nbsp;
      <span class="author-block">
        Hangrui Xu<sup>2</sup></span>,&nbsp; 
      <span class="author-block">
        Peng Jiao<sup>1</sup></span>,&nbsp;
      <span class="author-block">
        Haoqian Wang<sup>1</sup></span>&nbsp;
      <span class="author-block">
    </div>
    <div class="mb-4" style="text-align: center;">
      <span><sup>1</sup>Tsinghua University</span>,&nbsp;
      <span><sup>2</sup>Hefei University of Technology</span>&nbsp;
    </div>
    <div class="mb-4" style="text-align: center;">
      <span>* Equal Contribution</span>&nbsp;
    </div>

    <div class="w-100 d-flex flex-row justify-content-center mt-4 gap-2">
      <!-- Paper PDF -->
      <a href="https://arxiv.org/pdf/2503.18830" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    
      <!-- Code -->
      <a href="https://github.com/DingWu1021/DAGait" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>

      <!-- Dataset -->
      <!-- <a href="https://huggingface.co/datasets/xingyoujun/ss3d" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fa fa-database"></i>
        </span>
        <span>Dataset</span>
      </a> -->

      <!-- Video -->
      <!-- <a href="https://www.youtube.com/watch?v=MxZdNAy4EA4" target="_blank" class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fab fa-youtube"></i>
        </span>
        <span>Video</span>
      </a> -->

      <!-- Pre-trained Models -->
      <!-- <a href="https://huggingface.co/xingyoujun/transplat" target="_blank"
        class="btn btn-dark" role="button">
        <span class="icon">
          <i class="fas fa-database"></i>
        </span>
        <span>Pre-trained Models</span>
      </a> -->
    </div>

    <!-- Teaser -->
    <div class="w-100 my-4">
      <figure class="mb-4">
        <img src="static/images/baseline.png" class="img-fluid teaser" alt="architecture" />
        <figcaption style="font-size: smaller;text-align: justify;"><b>Overview of DAGait.</b> 
          In the data preprocessing stage, silhouettes and skeletons are extracted from the RGB image sequences. 
          During the data alignment phase, prior spatial relationships between skeleton joints and silhouette regions are used to apply an affine transformation, correcting the silhouette. 
          Finally, the aligned silhouette images are input into the backbone network for recognition.
        </figcaption>
      </figure>
    </div>

    <div class="main-contain">
      <!-- TL;DR -->
      <h2>TL;DR</h2>
      <div class="alert alert-success tldr mb-4">
        We present DAGait, a universal data alignment strategy for gait recognition, to alleviate spatiotemporal distribution inconsistencies.
      </div>

      <!-- Abstract -->
      <h2>Abstract</h2>
      <p class="mb-4" id="abstract">
        Gait recognition is emerging as a promising and innovative area within the field of computer vision, widely applied to remote person identification. 
        Although existing gait recognition methods have achieved substantial success in controlled laboratory datasets, their performance often declines significantly when transitioning to wild datasets.
        We argue that the performance gap can be primarily attributed to the spatio-temporal distribution inconsistencies present in wild datasets, where subjects appear at varying angles, positions, and distances across the frames. 
        To achieve accurate gait recognition in the wild, we propose a skeleton-guided silhouette alignment strategy, which uses prior knowledge of the skeletons to perform affine transformations on the corresponding silhouettes.
        To the best of our knowledge, this is the first study to explore the impact of data alignment on gait recognition. 
        We conducted extensive experiments across multiple datasets and network architectures, and the results demonstrate the significant advantages of our proposed alignment strategy.
        Specifically, on the challenging Gait3D dataset, our method achieved an average performance improvement of 7.9\% across all evaluated networks. 
        Furthermore, our method achieves substantial improvements on cross-domain datasets, with accuracy improvements of up to 24.0\%.
      </p>

      <h2>Introduction</h2>
      <figure class="mb-4">
        <img src="static/images/Intro.png" class="img-fluid teaser" alt="Intro" />
        <figcaption style="font-size: smaller;text-align: justify;"><b>we propose a gait recognition framework, named DAGait, designed to address the distribution discrepancies commonly observed in wild datasets.</b> 
          Accurate data alignment can mitigate interference from perspective shifts and posture variations, enabling the network to learn universal gait features.
          (a) The figure demonstrates the silhouette and gait energy image (GEI) within a sequence before and after data alignment, highlighting the alignment's effectiveness in mitigating spatio-temporal distribution inconsistencies. 
          (b) The performance comparison of GaitBase without and with data alignment across various datasets, demonstrating significant accuracy improvements, particularly on the Gait3D wild dataset.
        </figcaption>
      </figure>

      <!-- Comparisons -->
      <!-- <h2>Comparisons with the State-of-the-art</h2>
      <p>We present qualitative comparisons with the following state-of-the-art models:</p>
      <ul>
        <li><a href="https://davidcharatan.com/pixelsplat/">pixelSplat</a>: The latest feed-forward 3D Gaussians model with epipolar Transformer.</li>
      </ul>
      <ul>
        <li><a href="https://donydchen.github.io/mvsplat/">MVSplat</a>: The latest SOTA feed-forward 3D Gaussians model with costvolume.</li>
      </ul>
      <img src="static/images/sota_comparsion.png" class="img-fluid w-100 mt-2 mb-3" alt="SOTA comparisons" /> -->
      
      
      
      <!-- <div class="border w-100 mb-4">
        <video class="w-100 d-block" autoplay controls muted loop>
          <source src="static/videos/tod_video.mp4" type="video/mp4">
        </video>
      </div> -->

      <!-- pc results -->
      <h2>Comparisons with the State-of-the-art</h2> 
      <p>The proposed data alignment strategy consistently improves the performance of existing methods across all three datasets, with particularly significant effects observed on the more challenging wild dataset, Gait3D.</p>
      <img src="static/images/Sota.png" class="img-fluid w-100 mt-2 mb-3" alt="SOTA comparisons" />


      <h2>Visualization of Alignment</h2>
      <p>We present the visualizations of the aligned silhouettes and GEIs:</p>
      <div class="mudg-carousel">
        <div class="mudg-carousel-items">
          <div class="mudg-carousel-item is-active">
            <video class="carousel-video w-100 mt-2 mb-3" autoplay controls muted loop>
              <source src="static/Vedio/14-last.mp4" type="video/mp4">
            </video>
          </div>

        </div>

        <div class="mudg-carousel-nav">
          <button class="mudg-carousel-button prev"><</button>
          <button class="mudg-carousel-button next">></button>
        </div>

      </div>

      <!-- manipulation
      <h2>Cross-dataset Generalization </h2> 
      <p>Our proposed TranSplat demonstrates significant superiority in generalizing to out-of-distribution novel scenes.</p>
      <img src="static/images/cross_dataset.png" class="img-fluid w-100 mt-2 mb-3" alt="cross-dataset comparisons" /> -->

    <!-- <h2>BibTeX</h2>
    <pre class="mb-4"><code>@article{chen2024mvsplat,
    title   = {MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images},
    author  = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
    journal = {arXiv preprint arXiv:2403.14627},
    year    = {2024},
}</code></pre> -->

    </div>

    <!-- Footer -->
    <footer class="border-top mt-5 py-4">
      This page's code uses elements from this <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
        target="_blank">Academic Project Page
        Template</a>.
    </footer>
  </div>
</body>

</html>